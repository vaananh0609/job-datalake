import os
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator

# --- 1. Cáº¤U HÃŒNH MÃ”I TRÆ¯á»œNG & Káº¾T Ná»I SPARK ---
# Láº¥y biáº¿n mÃ´i trÆ°á»ng tá»« GitHub Actions workflow
AWS_ACCESS_KEY = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_REGION = os.getenv("AWS_REGION")
S3_ENDPOINT = os.getenv("S3_ENDPOINT", "https://s3.amazonaws.com")
BUCKET_NAME = os.getenv("S3_BUCKET_NAME")
# Prefix Ä‘áº§u vÃ o (vÃ­ dá»¥: processed/)
PREFIX_IN = os.getenv("S3_PREFIX", "processed/")

if not all([AWS_ACCESS_KEY, AWS_SECRET_KEY, BUCKET_NAME]):
    print("âŒ Lá»–I: Thiáº¿u biáº¿n mÃ´i trÆ°á»ng AWS/S3. Kiá»ƒm tra láº¡i GitHub Secrets.")
    sys.exit(1)

# Khá»Ÿi táº¡o Spark Session vá»›i cáº¥u hÃ¬nh S3A
spark = SparkSession.builder \
    .appName("TrainSalaryModel_CI") \
    .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4") \
    .config("spark.hadoop.fs.s3a.access.key", AWS_ACCESS_KEY) \
    .config("spark.hadoop.fs.s3a.secret.key", AWS_SECRET_KEY) \
    .config("spark.hadoop.fs.s3a.endpoint", S3_ENDPOINT) \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .getOrCreate()

print("ğŸš€ Spark Session Ä‘Ã£ khá»Ÿi táº¡o thÃ nh cÃ´ng!")

# --- 2. Äá»ŒC Dá»® LIá»†U Tá»ª S3 (SILVER LAYER) ---
# Äáº£m báº£o prefix cÃ³/khÃ´ng dáº¥u slash Ä‘á»u xá»­ lÃ½ Ä‘Ãºng
prefix_clean = PREFIX_IN.rstrip('/')
# ÄÆ°á»ng dáº«n file parquet Ä‘áº§u vÃ o (káº¿t quáº£ tá»« bÆ°á»›c ETL trÆ°á»›c)
input_path = f"s3a://{BUCKET_NAME}/{prefix_clean}/jobs_fact"
print(f"ğŸ“‚ Äang Ä‘á»c dá»¯ liá»‡u tá»«: {input_path}")

try:
    df = spark.read.parquet(input_path)
    # Chá»‰ láº¥y cÃ¡c báº£n ghi cÃ³ lÆ°Æ¡ng > 0 Ä‘á»ƒ train
    df_train_source = df.filter(col("salary_avg") > 0)
    print(f"ğŸ“Š Sá»‘ lÆ°á»£ng báº£n ghi há»£p lá»‡ Ä‘á»ƒ train: {df_train_source.count()}")
except Exception as e:
    print(f"âŒ Lá»—i Ä‘á»c file Parquet: {str(e)}")
    spark.stop()
    sys.exit(1)

# --- 3. XÃ‚Y Dá»°NG PIPELINE MACHINE LEARNING ---

# BÆ°á»›c A: Xá»­ lÃ½ dá»¯ liá»‡u Categorical (Biáº¿n chá»¯ thÃ nh sá»‘)
# setHandleInvalid="skip" Ä‘á»ƒ bá» qua cÃ¡c giÃ¡ trá»‹ má»›i láº¡ chÆ°a gáº·p lÃºc train
indexer_loc = StringIndexer(inputCol="location", outputCol="loc_idx", handleInvalid="skip")
indexer_lvl = StringIndexer(inputCol="level", outputCol="lvl_idx", handleInvalid="skip")

# BÆ°á»›c B: Gom cÃ¡c Ä‘áº·c trÆ°ng (Features) thÃ nh 1 vector
assembler = VectorAssembler(
    inputCols=["loc_idx", "lvl_idx"], # CÃ³ thá»ƒ thÃªm 'experience_years' náº¿u cÃ³
    outputCol="features"
)

# BÆ°á»›c C: Khai bÃ¡o thuáº­t toÃ¡n (Random Forest)
rf = RandomForestRegressor(featuresCol="features", labelCol="salary_avg", numTrees=50)

# Gom táº¥t cáº£ vÃ o 1 Pipeline
pipeline = Pipeline(stages=[indexer_loc, indexer_lvl, assembler, rf])

# --- 4. HUáº¤N LUYá»†N & ÄÃNH GIÃ ---
print("â³ Äang chia táº­p dá»¯ liá»‡u Train/Test...")
train_data, test_data = df_train_source.randomSplit([0.8, 0.2], seed=42)

print("ğŸ‹ï¸â€â™‚ï¸ Báº¯t Ä‘áº§u Training...")
model = pipeline.fit(train_data)

print("mag  Äang Evaluate trÃªn táº­p Test...")
predictions = model.transform(test_data)
evaluator = RegressionEvaluator(labelCol="salary_avg", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)

print("="*40)
print(f"âœ… Training HoÃ n táº¥t!")
print(f"ğŸ“‰ Sai sá»‘ trung bÃ¬nh (RMSE): {rmse:,.2f}")
print("="*40)

# --- 5. LÆ¯U MODEL (MODEL REGISTRY) ---
# LÆ°u model ra S3 Ä‘á»ƒ Web App (Streamlit/API) cÃ³ thá»ƒ táº£i vá» dÃ¹ng
model_output_path = f"s3a://{BUCKET_NAME}/models/salary_prediction_v1"
print(f"ğŸ’¾ Äang lÆ°u model vÃ o: {model_output_path}")

try:
    model.write().overwrite().save(model_output_path)
    print("âœ… LÆ°u Model thÃ nh cÃ´ng!")
except Exception as e:
    print(f"âŒ Lá»—i khi lÆ°u model: {str(e)}")
    sys.exit(1)

spark.stop()
